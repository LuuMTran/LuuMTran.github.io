1:"$Sreact.fragment"
2:I[54381,["/_next/static/chunks/adf3234c86b72dd4.js","/_next/static/chunks/9e4c325896390388.js"],"BlogPostContent"]
3:I[99760,["/_next/static/chunks/d5f7520d963047ce.js","/_next/static/chunks/41f4bd048becbe7c.js"],"OutletBoundary"]
4:"$Sreact.suspense"
0:{"buildId":"0MiwC0RnazdcUmcQSodAT","rsc":["$","$1","c",{"children":[["$","$L2",null,{"post":{"slug":"deploying-ml-models","title":"Deploying ML Models to Production: A Practical Guide","excerpt":"Learn how to deploy machine learning models using FastAPI, Docker, and cloud platforms with CI/CD pipelines.","content":"\n# Deploying ML Models to Production\n\nBuilding a model is only half the battle. Deploying it to production requires careful consideration of many factors.\n\n## Deployment Options\n\n### 1. REST API with FastAPI\nFastAPI is perfect for ML model serving because:\n- High performance (async)\n- Automatic API documentation\n- Easy to use\n\n### 2. Batch Processing\nFor scenarios where real-time isn't needed...\n\n[Full content continues]\n    ","date":"2023-12-20","readTime":"11 min read","category":"MLOps","tags":["Deployment","FastAPI","Docker","CI/CD"]}}],[["$","script","script-0",{"src":"/_next/static/chunks/adf3234c86b72dd4.js","async":true}],["$","script","script-1",{"src":"/_next/static/chunks/9e4c325896390388.js","async":true}]],["$","$L3",null,{"children":["$","$4",null,{"name":"Next.MetadataOutlet","children":"$@5"}]}]]}],"loading":null,"isPartial":false}
5:null
