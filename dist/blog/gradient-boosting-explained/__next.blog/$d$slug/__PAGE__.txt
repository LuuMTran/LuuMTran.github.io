1:"$Sreact.fragment"
2:I[54381,["/_next/static/chunks/26e8826b9a0ceccc.js","/_next/static/chunks/2ad9216ef6accbba.js"],"BlogPostContent"]
3:I[99760,["/_next/static/chunks/d5f7520d963047ce.js","/_next/static/chunks/41f4bd048becbe7c.js"],"OutletBoundary"]
4:"$Sreact.suspense"
0:{"buildId":"VPy-Ocix7WusGrEyEZ5dZ","rsc":["$","$1","c",{"children":[["$","$L2",null,{"post":{"slug":"gradient-boosting-explained","title":"Gradient Boosting Explained: XGBoost, LightGBM, CatBoost","excerpt":"Deep dive into gradient boosting algorithms and when to use each variant for maximum performance.","content":"\n# Gradient Boosting Explained\n\nGradient boosting machines have dominated tabular data competitions for years. Let's understand why.\n\n## Ensemble Methods\n\nThree main types:\n1. Bagging (Random Forest)\n2. Boosting (AdaBoost, Gradient Boosting)\n3. Stacking\n\n## How Gradient Boosting Works\n\nThe key idea: train models sequentially, each correcting errors of previous ones...\n\n[Full content continues]\n    ","date":"2023-12-15","readTime":"9 min read","category":"Machine Learning","tags":["XGBoost","LightGBM","Gradient Boosting","Ensemble"]}}],[["$","script","script-0",{"src":"/_next/static/chunks/26e8826b9a0ceccc.js","async":true}],["$","script","script-1",{"src":"/_next/static/chunks/2ad9216ef6accbba.js","async":true}]],["$","$L3",null,{"children":["$","$4",null,{"name":"Next.MetadataOutlet","children":"$@5"}]}]]}],"loading":null,"isPartial":false}
5:null
