<!DOCTYPE html><!--VPy_Ocix7WusGrEyEZ5dZ--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/chunks/6bb39ef18f9527a9.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/7214ae92498faad0.js"/><script src="/_next/static/chunks/e7921d94f0207552.js" async=""></script><script src="/_next/static/chunks/57fa03b4926e0413.js" async=""></script><script src="/_next/static/chunks/df16c55faaaa69b9.js" async=""></script><script src="/_next/static/chunks/turbopack-a067586eaf39226a.js" async=""></script><script src="/_next/static/chunks/d5f7520d963047ce.js" async=""></script><script src="/_next/static/chunks/41f4bd048becbe7c.js" async=""></script><script src="/_next/static/chunks/26e8826b9a0ceccc.js" async=""></script><script src="/_next/static/chunks/2ad9216ef6accbba.js" async=""></script><meta name="next-size-adjust" content=""/><title>Luu Minh Thong Tran</title><meta name="description" content="Portfolio of Luu Minh Thong Tran - AI/ML Engineer"/><script>
              (function() {
                const savedTheme = localStorage.getItem('theme');
                // Default to light mode unless user explicitly chose dark
                const theme = savedTheme || 'light';
                
                if (theme === 'dark') {
                  document.documentElement.classList.add('dark');
                }
                document.documentElement.setAttribute('data-theme', theme);
              })();
            </script><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased bg-background text-foreground"><div hidden=""><!--$--><!--/$--></div><main class="relative min-h-screen bg-background"><div class="fixed inset-0 w-full h-full overflow-hidden -z-10 animated-grid" style="background-image:linear-gradient(to right, var(--border) 1px, transparent 1px),
          linear-gradient(to bottom, var(--border) 1px, transparent 1px);background-size:40px 40px"></div><header class="sticky top-0 z-50 bg-card/80 backdrop-blur-md border-b border-border"><div class="max-w-4xl mx-auto px-4 md:px-8 py-4 flex items-center justify-between"><a class="flex items-center gap-2 text-muted-foreground hover:text-primary transition-colors" href="/blog"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-left w-4 h-4" aria-hidden="true"><path d="m12 19-7-7 7-7"></path><path d="M19 12H5"></path></svg><span class="font-medium">Back to Blog</span></a><div class="flex items-center gap-2"><div class="w-8 h-8 relative"><img alt="Luu Tran Logo" loading="lazy" decoding="async" data-nimg="fill" class="object-contain" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/logo.svg"/></div><span class="font-bold text-foreground">Blog</span></div></div></header><article class="pt-8 pb-16 px-4 md:px-8"><div class="max-w-4xl mx-auto"><div class="mb-8" style="opacity:0;transform:translateY(20px)"><div class="inline-block px-3 py-1 rounded-full bg-primary-light border border-border mb-4"><span class="text-sm font-medium text-primary">NLP</span></div><h1 class="text-3xl md:text-4xl lg:text-5xl font-bold text-foreground mb-6">Transformers Explained: A Visual Guide</h1><div class="flex flex-wrap items-center gap-4 text-sm text-muted-foreground mb-6"><div class="flex items-center gap-1"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-calendar w-4 h-4" aria-hidden="true"><path d="M8 2v4"></path><path d="M16 2v4"></path><rect width="18" height="18" x="3" y="4" rx="2"></rect><path d="M3 10h18"></path></svg>2024-01-10</div><div class="flex items-center gap-1"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-clock w-4 h-4" aria-hidden="true"><path d="M12 6v6l4 2"></path><circle cx="12" cy="12" r="10"></circle></svg>12 min read</div></div><div class="flex flex-wrap gap-2 mb-8"><span class="px-3 py-1 rounded-full bg-muted border border-border text-sm text-muted-foreground">Transformers</span><span class="px-3 py-1 rounded-full bg-muted border border-border text-sm text-muted-foreground">BERT</span><span class="px-3 py-1 rounded-full bg-muted border border-border text-sm text-muted-foreground">GPT</span><span class="px-3 py-1 rounded-full bg-muted border border-border text-sm text-muted-foreground">Attention Mechanism</span></div><div class="flex items-center gap-3 pt-4 border-t border-border"><span class="text-sm text-muted-foreground">Share:</span><button class="p-2 rounded-full border border-border hover:bg-primary-light hover:border-primary hover:text-primary transition-all"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-twitter w-4 h-4" aria-hidden="true"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></button><button class="p-2 rounded-full border border-border hover:bg-primary-light hover:border-primary hover:text-primary transition-all"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-linkedin w-4 h-4" aria-hidden="true"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect width="4" height="12" x="2" y="9"></rect><circle cx="4" cy="4" r="2"></circle></svg></button><button class="p-2 rounded-full border border-border hover:bg-primary-light hover:border-primary hover:text-primary transition-all"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-share2 lucide-share-2 w-4 h-4" aria-hidden="true"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" x2="15.42" y1="13.51" y2="17.49"></line><line x1="15.41" x2="8.59" y1="6.51" y2="10.49"></line></svg></button></div></div><div class="prose prose-lg max-w-none" style="opacity:0;transform:translateY(20px)"><div class="bg-card rounded-2xl p-8 md:p-12 border border-border shadow-sm"><br /><h1 class="text-3xl font-bold text-foreground mt-8 mb-4">Transformers Explained: A Visual Guide</h1><br /><p class="text-muted-foreground leading-relaxed mb-4">Transformers have revolutionized natural language processing since their introduction in the "Attention Is All You Need" paper. Let's break down how they work.</p><br /><h2 class="text-2xl font-bold text-foreground mt-6 mb-3">The Problem with RNNs</h2><br /><p class="text-muted-foreground leading-relaxed mb-4">Recurrent Neural Networks process sequences one element at a time, which makes them:</p><p class="text-muted-foreground leading-relaxed mb-4">- Slow to train (sequential processing)</p><p class="text-muted-foreground leading-relaxed mb-4">- Prone to vanishing gradients</p><p class="text-muted-foreground leading-relaxed mb-4">- Unable to capture long-range dependencies effectively</p><br /><h2 class="text-2xl font-bold text-foreground mt-6 mb-3">The Transformer Solution</h2><br /><p class="text-muted-foreground leading-relaxed mb-4">Transformers solve these issues through:</p><p class="text-muted-foreground leading-relaxed mb-4">1. **Self-Attention**: Every token can attend to every other token</p><p class="text-muted-foreground leading-relaxed mb-4">2. **Parallel Processing**: No sequential dependencies</p><p class="text-muted-foreground leading-relaxed mb-4">3. **Positional Encoding**: Injecting sequence order information</p><br /><h2 class="text-2xl font-bold text-foreground mt-6 mb-3">Architecture Overview</h2><br /><h3 class="text-xl font-bold text-foreground mt-4 mb-2">Encoder Stack</h3><p class="text-muted-foreground leading-relaxed mb-4">The encoder processes input tokens and creates rich representations...</p><br /><br /></div></div><div class="mt-12 flex items-center justify-between pt-8 border-t border-border" style="opacity:0"><a class="flex items-center gap-2 text-muted-foreground hover:text-primary transition-colors" href="/blog"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-left w-4 h-4" aria-hidden="true"><path d="m12 19-7-7 7-7"></path><path d="M19 12H5"></path></svg><span>All Articles</span></a></div></div></article><footer class="py-8 px-4 border-t border-border bg-background"><div class="max-w-4xl mx-auto text-center"><p class="text-sm text-muted-foreground">Â© 2024 Luu Minh Thong Tran</p></div></footer></main><!--$--><!--/$--><script src="/_next/static/chunks/7214ae92498faad0.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[45614,[\"/_next/static/chunks/d5f7520d963047ce.js\",\"/_next/static/chunks/41f4bd048becbe7c.js\"],\"default\"]\n3:I[66827,[\"/_next/static/chunks/d5f7520d963047ce.js\",\"/_next/static/chunks/41f4bd048becbe7c.js\"],\"default\"]\n5:I[99760,[\"/_next/static/chunks/d5f7520d963047ce.js\",\"/_next/static/chunks/41f4bd048becbe7c.js\"],\"OutletBoundary\"]\n6:\"$Sreact.suspense\"\n8:I[99760,[\"/_next/static/chunks/d5f7520d963047ce.js\",\"/_next/static/chunks/41f4bd048becbe7c.js\"],\"ViewportBoundary\"]\na:I[99760,[\"/_next/static/chunks/d5f7520d963047ce.js\",\"/_next/static/chunks/41f4bd048becbe7c.js\"],\"MetadataBoundary\"]\nc:I[77369,[\"/_next/static/chunks/d5f7520d963047ce.js\",\"/_next/static/chunks/41f4bd048becbe7c.js\"],\"default\"]\n:HL[\"/_next/static/chunks/6bb39ef18f9527a9.css\",\"style\"]\n:HL[\"/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"VPy-Ocix7WusGrEyEZ5dZ\",\"c\":[\"\",\"blog\",\"transformers-explained\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"transformers-explained\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/6bb39ef18f9527a9.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              (function() {\\n                const savedTheme = localStorage.getItem('theme');\\n                // Default to light mode unless user explicitly chose dark\\n                const theme = savedTheme || 'light';\\n                \\n                if (theme === 'dark') {\\n                  document.documentElement.classList.add('dark');\\n                }\\n                document.documentElement.setAttribute('data-theme', theme);\\n              })();\\n            \"}}]}],[\"$\",\"body\",null,{\"className\":\"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased bg-background text-foreground\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L4\",[[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/26e8826b9a0ceccc.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/2ad9216ef6accbba.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L5\",null,{\"children\":[\"$\",\"$6\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@7\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$La\",null,{\"children\":[\"$\",\"$6\",null,{\"name\":\"Next.Metadata\",\"children\":\"$Lb\"}]}]}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$c\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"d:I[54381,[\"/_next/static/chunks/26e8826b9a0ceccc.js\",\"/_next/static/chunks/2ad9216ef6accbba.js\"],\"BlogPostContent\"]\n4:[\"$\",\"$Ld\",null,{\"post\":{\"slug\":\"transformers-explained\",\"title\":\"Transformers Explained: A Visual Guide\",\"excerpt\":\"A comprehensive visual explanation of the Transformer architecture that powers GPT, BERT, and modern NLP systems.\",\"content\":\"\\n# Transformers Explained: A Visual Guide\\n\\nTransformers have revolutionized natural language processing since their introduction in the \\\"Attention Is All You Need\\\" paper. Let's break down how they work.\\n\\n## The Problem with RNNs\\n\\nRecurrent Neural Networks process sequences one element at a time, which makes them:\\n- Slow to train (sequential processing)\\n- Prone to vanishing gradients\\n- Unable to capture long-range dependencies effectively\\n\\n## The Transformer Solution\\n\\nTransformers solve these issues through:\\n1. **Self-Attention**: Every token can attend to every other token\\n2. **Parallel Processing**: No sequential dependencies\\n3. **Positional Encoding**: Injecting sequence order information\\n\\n## Architecture Overview\\n\\n### Encoder Stack\\nThe encoder processes input tokens and creates rich representations...\\n\\n[Full content continues]\\n    \",\"date\":\"2024-01-10\",\"readTime\":\"12 min read\",\"category\":\"NLP\",\"tags\":[\"Transformers\",\"BERT\",\"GPT\",\"Attention Mechanism\"],\"featured\":true}}]\n"])</script><script>self.__next_f.push([1,"9:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"7:null\nb:[[\"$\",\"title\",\"0\",{\"children\":\"Luu Minh Thong Tran\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Portfolio of Luu Minh Thong Tran - AI/ML Engineer\"}]]\n"])</script></body></html>