1:"$Sreact.fragment"
2:I[54381,["/_next/static/chunks/26e8826b9a0ceccc.js","/_next/static/chunks/2ad9216ef6accbba.js"],"BlogPostContent"]
3:I[99760,["/_next/static/chunks/d5f7520d963047ce.js","/_next/static/chunks/41f4bd048becbe7c.js"],"OutletBoundary"]
4:"$Sreact.suspense"
0:{"buildId":"VPy-Ocix7WusGrEyEZ5dZ","rsc":["$","$1","c",{"children":[["$","$L2",null,{"post":{"slug":"transformers-explained","title":"Transformers Explained: A Visual Guide","excerpt":"A comprehensive visual explanation of the Transformer architecture that powers GPT, BERT, and modern NLP systems.","content":"\n# Transformers Explained: A Visual Guide\n\nTransformers have revolutionized natural language processing since their introduction in the \"Attention Is All You Need\" paper. Let's break down how they work.\n\n## The Problem with RNNs\n\nRecurrent Neural Networks process sequences one element at a time, which makes them:\n- Slow to train (sequential processing)\n- Prone to vanishing gradients\n- Unable to capture long-range dependencies effectively\n\n## The Transformer Solution\n\nTransformers solve these issues through:\n1. **Self-Attention**: Every token can attend to every other token\n2. **Parallel Processing**: No sequential dependencies\n3. **Positional Encoding**: Injecting sequence order information\n\n## Architecture Overview\n\n### Encoder Stack\nThe encoder processes input tokens and creates rich representations...\n\n[Full content continues]\n    ","date":"2024-01-10","readTime":"12 min read","category":"NLP","tags":["Transformers","BERT","GPT","Attention Mechanism"],"featured":true}}],[["$","script","script-0",{"src":"/_next/static/chunks/26e8826b9a0ceccc.js","async":true}],["$","script","script-1",{"src":"/_next/static/chunks/2ad9216ef6accbba.js","async":true}]],["$","$L3",null,{"children":["$","$4",null,{"name":"Next.MetadataOutlet","children":"$@5"}]}]]}],"loading":null,"isPartial":false}
5:null
