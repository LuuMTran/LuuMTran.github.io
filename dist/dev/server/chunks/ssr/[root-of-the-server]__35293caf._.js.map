{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 14, "column": 0}, "map": {"version":3,"sources":["file:///C:/Users/thanh/OneDrive%20-%20UTS/Personal_code/thong-portfolio/src/data/blog.ts"],"sourcesContent":["// Blog posts data\r\nexport interface BlogPost {\r\n  slug: string;\r\n  title: string;\r\n  excerpt: string;\r\n  content: string;\r\n  date: string;\r\n  readTime: string;\r\n  category: string;\r\n  tags: string[];\r\n  featured?: boolean;\r\n}\r\n\r\nexport const blogPosts: BlogPost[] = [\r\n  {\r\n    slug: \"neural-network-from-scratch\",\r\n    title: \"Building a Neural Network from Scratch\",\r\n    excerpt: \"Learn how to implement backpropagation and gradient descent to build a fully functional neural network using only NumPy.\",\r\n    content: `\r\n# Building a Neural Network from Scratch\r\n\r\nIn this comprehensive guide, we'll build a neural network from scratch using only NumPy. This exercise will give you deep insights into how neural networks actually work under the hood.\r\n\r\n## Why Build From Scratch?\r\n\r\nUnderstanding the fundamentals is crucial for any ML engineer. While frameworks like TensorFlow and PyTorch abstract away the details, knowing what's happening internally helps you:\r\n- Debug issues more effectively\r\n- Optimize architectures better\r\n- Understand the mathematics behind deep learning\r\n\r\n## The Architecture\r\n\r\nWe'll build a simple feedforward neural network with:\r\n- Input layer\r\n- Two hidden layers with ReLU activation\r\n- Output layer with softmax\r\n- Cross-entropy loss function\r\n\r\n## Implementation\r\n\r\n### Step 1: Initialize Parameters\r\n\\`\\`\\`python\r\nimport numpy as np\r\n\r\ndef initialize_parameters(layer_dims):\r\n    parameters = {}\r\n    L = len(layer_dims)\r\n    \r\n    for l in range(1, L):\r\n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\r\n        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\r\n    \r\n    return parameters\r\n\\`\\`\\`\r\n\r\n### Step 2: Forward Propagation\r\nThe forward pass computes the output of the network...\r\n\r\n[Continue with full implementation details]\r\n    `,\r\n    date: \"2024-01-15\",\r\n    readTime: \"8 min read\",\r\n    category: \"Deep Learning\",\r\n    tags: [\"Neural Networks\", \"Python\", \"NumPy\", \"Tutorial\"],\r\n    featured: true,\r\n  },\r\n  {\r\n    slug: \"transformers-explained\",\r\n    title: \"Transformers Explained: A Visual Guide\",\r\n    excerpt: \"A comprehensive visual explanation of the Transformer architecture that powers GPT, BERT, and modern NLP systems.\",\r\n    content: `\r\n# Transformers Explained: A Visual Guide\r\n\r\nTransformers have revolutionized natural language processing since their introduction in the \"Attention Is All You Need\" paper. Let's break down how they work.\r\n\r\n## The Problem with RNNs\r\n\r\nRecurrent Neural Networks process sequences one element at a time, which makes them:\r\n- Slow to train (sequential processing)\r\n- Prone to vanishing gradients\r\n- Unable to capture long-range dependencies effectively\r\n\r\n## The Transformer Solution\r\n\r\nTransformers solve these issues through:\r\n1. **Self-Attention**: Every token can attend to every other token\r\n2. **Parallel Processing**: No sequential dependencies\r\n3. **Positional Encoding**: Injecting sequence order information\r\n\r\n## Architecture Overview\r\n\r\n### Encoder Stack\r\nThe encoder processes input tokens and creates rich representations...\r\n\r\n[Full content continues]\r\n    `,\r\n    date: \"2024-01-10\",\r\n    readTime: \"12 min read\",\r\n    category: \"NLP\",\r\n    tags: [\"Transformers\", \"BERT\", \"GPT\", \"Attention Mechanism\"],\r\n    featured: true,\r\n  },\r\n  {\r\n    slug: \"hyperparameter-tuning-guide\",\r\n    title: \"The Complete Guide to Hyperparameter Tuning\",\r\n    excerpt: \"Master hyperparameter optimization techniques including Grid Search, Random Search, Bayesian Optimization, and Hyperband.\",\r\n    content: `\r\n# The Complete Guide to Hyperparameter Tuning\r\n\r\nHyperparameter tuning is both an art and a science. This guide covers all major techniques from basic to advanced.\r\n\r\n## What Are Hyperparameters?\r\n\r\nUnlike model parameters (weights), hyperparameters are:\r\n- Learning rate\r\n- Batch size\r\n- Number of layers\r\n- Number of units per layer\r\n- Regularization strength\r\n\r\n## Manual Tuning\r\n\r\nStart with understanding each hyperparameter's effect...\r\n\r\n[Full content continues]\r\n    `,\r\n    date: \"2024-01-05\",\r\n    readTime: \"10 min read\",\r\n    category: \"MLOps\",\r\n    tags: [\"Hyperparameter Tuning\", \"Bayesian Optimization\", \"Optuna\"],\r\n  },\r\n  {\r\n    slug: \"computer-vision-cnn\",\r\n    title: \"Computer Vision with CNNs: From Theory to Practice\",\r\n    excerpt: \"Learn how Convolutional Neural Networks process images and build your own image classifier from scratch.\",\r\n    content: `\r\n# Computer Vision with CNNs\r\n\r\nConvolutional Neural Networks are the backbone of modern computer vision. Let's understand how they work and build one ourselves.\r\n\r\n## Why CNNs for Images?\r\n\r\nImages have special properties:\r\n- Spatial structure matters\r\n- Nearby pixels are correlated\r\n- Features are translation invariant\r\n\r\n## The Convolution Operation\r\n\r\nAt the heart of CNNs is the convolution operation...\r\n\r\n[Full content continues]\r\n    `,\r\n    date: \"2023-12-28\",\r\n    readTime: \"15 min read\",\r\n    category: \"Computer Vision\",\r\n    tags: [\"CNN\", \"Computer Vision\", \"Image Classification\", \"OpenCV\"],\r\n  },\r\n  {\r\n    slug: \"deploying-ml-models\",\r\n    title: \"Deploying ML Models to Production: A Practical Guide\",\r\n    excerpt: \"Learn how to deploy machine learning models using FastAPI, Docker, and cloud platforms with CI/CD pipelines.\",\r\n    content: `\r\n# Deploying ML Models to Production\r\n\r\nBuilding a model is only half the battle. Deploying it to production requires careful consideration of many factors.\r\n\r\n## Deployment Options\r\n\r\n### 1. REST API with FastAPI\r\nFastAPI is perfect for ML model serving because:\r\n- High performance (async)\r\n- Automatic API documentation\r\n- Easy to use\r\n\r\n### 2. Batch Processing\r\nFor scenarios where real-time isn't needed...\r\n\r\n[Full content continues]\r\n    `,\r\n    date: \"2023-12-20\",\r\n    readTime: \"11 min read\",\r\n    category: \"MLOps\",\r\n    tags: [\"Deployment\", \"FastAPI\", \"Docker\", \"CI/CD\"],\r\n  },\r\n  {\r\n    slug: \"gradient-boosting-explained\",\r\n    title: \"Gradient Boosting Explained: XGBoost, LightGBM, CatBoost\",\r\n    excerpt: \"Deep dive into gradient boosting algorithms and when to use each variant for maximum performance.\",\r\n    content: `\r\n# Gradient Boosting Explained\r\n\r\nGradient boosting machines have dominated tabular data competitions for years. Let's understand why.\r\n\r\n## Ensemble Methods\r\n\r\nThree main types:\r\n1. Bagging (Random Forest)\r\n2. Boosting (AdaBoost, Gradient Boosting)\r\n3. Stacking\r\n\r\n## How Gradient Boosting Works\r\n\r\nThe key idea: train models sequentially, each correcting errors of previous ones...\r\n\r\n[Full content continues]\r\n    `,\r\n    date: \"2023-12-15\",\r\n    readTime: \"9 min read\",\r\n    category: \"Machine Learning\",\r\n    tags: [\"XGBoost\", \"LightGBM\", \"Gradient Boosting\", \"Ensemble\"],\r\n  },\r\n];\r\n\r\nexport function getBlogPostBySlug(slug: string): BlogPost | undefined {\r\n  return blogPosts.find((post) => post.slug === slug);\r\n}\r\n\r\nexport function getFeaturedPosts(): BlogPost[] {\r\n  return blogPosts.filter((post) => post.featured);\r\n}\r\n\r\nexport function getAllCategories(): string[] {\r\n  const categories = new Set(blogPosts.map((post) => post.category));\r\n  return Array.from(categories);\r\n}\r\n"],"names":[],"mappings":"AAAA,kBAAkB;;;;;;;;;;;AAaX,MAAM,YAAwB;IACnC;QACE,MAAM;QACN,OAAO;QACP,SAAS;QACT,SAAS,CAAC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;IAyCV,CAAC;QACD,MAAM;QACN,UAAU;QACV,UAAU;QACV,MAAM;YAAC;YAAmB;YAAU;YAAS;SAAW;QACxD,UAAU;IACZ;IACA;QACE,MAAM;QACN,OAAO;QACP,SAAS;QACT,SAAS,CAAC;;;;;;;;;;;;;;;;;;;;;;;;;IAyBV,CAAC;QACD,MAAM;QACN,UAAU;QACV,UAAU;QACV,MAAM;YAAC;YAAgB;YAAQ;YAAO;SAAsB;QAC5D,UAAU;IACZ;IACA;QACE,MAAM;QACN,OAAO;QACP,SAAS;QACT,SAAS,CAAC;;;;;;;;;;;;;;;;;;;IAmBV,CAAC;QACD,MAAM;QACN,UAAU;QACV,UAAU;QACV,MAAM;YAAC;YAAyB;YAAyB;SAAS;IACpE;IACA;QACE,MAAM;QACN,OAAO;QACP,SAAS;QACT,SAAS,CAAC;;;;;;;;;;;;;;;;;IAiBV,CAAC;QACD,MAAM;QACN,UAAU;QACV,UAAU;QACV,MAAM;YAAC;YAAO;YAAmB;YAAwB;SAAS;IACpE;IACA;QACE,MAAM;QACN,OAAO;QACP,SAAS;QACT,SAAS,CAAC;;;;;;;;;;;;;;;;;IAiBV,CAAC;QACD,MAAM;QACN,UAAU;QACV,UAAU;QACV,MAAM;YAAC;YAAc;YAAW;YAAU;SAAQ;IACpD;IACA;QACE,MAAM;QACN,OAAO;QACP,SAAS;QACT,SAAS,CAAC;;;;;;;;;;;;;;;;;IAiBV,CAAC;QACD,MAAM;QACN,UAAU;QACV,UAAU;QACV,MAAM;YAAC;YAAW;YAAY;YAAqB;SAAW;IAChE;CACD;AAEM,SAAS,kBAAkB,IAAY;IAC5C,OAAO,UAAU,IAAI,CAAC,CAAC,OAAS,KAAK,IAAI,KAAK;AAChD;AAEO,SAAS;IACd,OAAO,UAAU,MAAM,CAAC,CAAC,OAAS,KAAK,QAAQ;AACjD;AAEO,SAAS;IACd,MAAM,aAAa,IAAI,IAAI,UAAU,GAAG,CAAC,CAAC,OAAS,KAAK,QAAQ;IAChE,OAAO,MAAM,IAAI,CAAC;AACpB"}},
    {"offset": {"line": 268, "column": 0}, "map": {"version":3,"sources":["file:///C:/Users/thanh/OneDrive%20-%20UTS/Personal_code/thong-portfolio/src/components/BlogPostContent.tsx/__nextjs-internal-proxy.mjs"],"sourcesContent":["// This file is generated by next-core EcmascriptClientReferenceModule.\nimport { registerClientReference } from \"react-server-dom-turbopack/server\";\nexport const BlogPostContent = registerClientReference(\n    function() { throw new Error(\"Attempted to call BlogPostContent() from the server but BlogPostContent is on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/OneDrive - UTS/Personal_code/thong-portfolio/src/components/BlogPostContent.tsx <module evaluation>\",\n    \"BlogPostContent\",\n);\n"],"names":[],"mappings":";;;;AAAA,uEAAuE;AACvE;;AACO,MAAM,kBAAkB,IAAA,sUAAuB,EAClD;IAAa,MAAM,IAAI,MAAM;AAA8O,GAC3Q,iHACA","ignoreList":[0]}},
    {"offset": {"line": 282, "column": 0}, "map": {"version":3,"sources":["file:///C:/Users/thanh/OneDrive%20-%20UTS/Personal_code/thong-portfolio/src/components/BlogPostContent.tsx/__nextjs-internal-proxy.mjs"],"sourcesContent":["// This file is generated by next-core EcmascriptClientReferenceModule.\nimport { registerClientReference } from \"react-server-dom-turbopack/server\";\nexport const BlogPostContent = registerClientReference(\n    function() { throw new Error(\"Attempted to call BlogPostContent() from the server but BlogPostContent is on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/OneDrive - UTS/Personal_code/thong-portfolio/src/components/BlogPostContent.tsx\",\n    \"BlogPostContent\",\n);\n"],"names":[],"mappings":";;;;AAAA,uEAAuE;AACvE;;AACO,MAAM,kBAAkB,IAAA,sUAAuB,EAClD;IAAa,MAAM,IAAI,MAAM;AAA8O,GAC3Q,6FACA","ignoreList":[0]}},
    {"offset": {"line": 296, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":""}},
    {"offset": {"line": 304, "column": 0}, "map": {"version":3,"sources":["file:///C:/Users/thanh/OneDrive%20-%20UTS/Personal_code/thong-portfolio/src/app/blog/%5Bslug%5D/page.tsx"],"sourcesContent":["import { notFound } from \"next/navigation\";\r\nimport { getBlogPostBySlug, blogPosts } from \"@/data/blog\";\r\nimport { BlogPostContent } from \"@/components/BlogPostContent\";\r\n\r\ninterface BlogPostPageProps {\r\n  params: Promise<{\r\n    slug: string;\r\n  }>;\r\n}\r\n\r\n// Server Component\r\nexport default async function BlogPostPage({ params }: BlogPostPageProps) {\r\n  const { slug } = await params;\r\n  const post = getBlogPostBySlug(slug);\r\n\r\n  if (!post) {\r\n    notFound();\r\n  }\r\n\r\n  return <BlogPostContent post={post} />;\r\n}\r\n\r\n// Generate static paths for all blog posts\r\nexport function generateStaticParams() {\r\n  return blogPosts.map((post) => ({\r\n    slug: post.slug,\r\n  }));\r\n}\r\n"],"names":[],"mappings":";;;;;;;AAAA;AAAA;AACA;AACA;;;;;AASe,eAAe,aAAa,EAAE,MAAM,EAAqB;IACtE,MAAM,EAAE,IAAI,EAAE,GAAG,MAAM;IACvB,MAAM,OAAO,IAAA,sMAAiB,EAAC;IAE/B,IAAI,CAAC,MAAM;QACT,IAAA,+PAAQ;IACV;IAEA,qBAAO,4SAAC,sNAAe;QAAC,MAAM;;;;;;AAChC;AAGO,SAAS;IACd,OAAO,8LAAS,CAAC,GAAG,CAAC,CAAC,OAAS,CAAC;YAC9B,MAAM,KAAK,IAAI;QACjB,CAAC;AACH"}}]
}