{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 14, "column": 0}, "map": {"version":3,"sources":["file:///C:/Users/thanh/OneDrive%20-%20UTS/Personal_code/thong-portfolio/src/data/blog.ts"],"sourcesContent":["// Blog posts data\nexport interface BlogPost {\n  slug: string;\n  title: string;\n  excerpt: string;\n  content: string;\n  date: string;\n  readTime: string;\n  category: string;\n  tags: string[];\n  featured?: boolean;\n}\n\nexport const blogPosts: BlogPost[] = [\n  {\n    slug: \"in-depth-svm-visualizations\",\n    title: \"In-depth SVM with Visualizations\",\n    excerpt: \"A comprehensive deep dive into Support Vector Machines, from loss functions and primal/dual forms to kernel tricks and SMO optimization, complete with visual explanations.\",\n    content: `\n# In-depth SVM with visualisations \n\n## Background\n\nI have been studying traditional classification machines and learning algorithms. It grinded through different kinds of models from linear-family models to complicated methods such as ensemble methods. I understanded quite well until I met Support Vector Machines.\n\nOn the surface, I can understand quite well what struck me is its optimization algorithms. Unlike other models, optimization is based on gradients or entropy for trees. However, the way SVM optimizes its weight works quite differently, which I will explain later on.\n\n\n## Introduction\n\nOk let look at what is Support Vector Machine and what is does. Imagine there are 2 groups of people wants to divde the land equally by drawing a straight line, SVM will help them equally between them. Now, bring it to the context of machine learning, a SVM will find a hyperplane that seperate 2 classes with maximal margin (equal land). \n\nHave a lookt at this decision boundary (although it is not linear but we get to that part). it perfectly give the decision boundary right in the midde!!\n\n\n![alt text](/images/svm-blog/image.png)\n\n\n\nHmmm... seems easy right? \n\nWell then how it do it? Let's look at the loss function:\n\n### Loss Function\n\n$$\n\\\\begin{aligned}\\\\min_{w, b} \\\\; \\\\frac{1}{2} \\\\lVert w \\\\rVert^2 + C \\\\sum_{i=1}^{n} \\\\max \\\\left( 0, \\\\; 1 - y_i \\\\left( w^T x_i + b \\\\right) \\\\right)\\\\end{aligned}\n$$\n\n\nThe term $\\\\max \\\\left( 0, \\\\; 1 - y_i \\\\left( w^T x_i + b \\\\right) \\\\right)$ seems familar right? Thats right!! It's the perceptron loss! It try to minimse the perceptron loss meaning it try to  correctly classify the samples! What about the first term? Well its the regularization term which means it try to divide the land equally! Finally, the C controls how you would want the classfier to focus on correctly classify or divide the land equally.\n\n### Primal-form \n\nWARNING: Math-heavy, you can skip this and belive that the loss function has the primal form as the equation below (same thing but different notation)\n\n\nBy transforming the objective function to produce a set constraint, it can be later manipulated into the dual form. The un-constrained objective function can be re-written as:\n\n$$\n\\\\min_{w, b, \\\\xi} \\\\; \\\\frac{1}{2} \\\\lVert w \\\\rVert^2 + C \\\\sum_{i=1}^{n} \\\\xi_i\\\\quad \\\\text{s.t.} \\\\quad y_i (w^T x_i + b) \\\\geq 1 - \\\\xi_i, \\\\quad \\\\xi_i \\\\geq 0\n$$\n\nWhere $\\\\xi_i$ and the constraint \n$y_i (w^T x_i + b) \\\\geq 1 - \\\\xi_i, \\\\; \\\\xi_i \\\\geq 0$ \nis equivalent to \n$\\\\max \\\\left( 0, 1 - y_i (w^T x_i + b) \\\\right)$. \nBy introducing these constraints, the problem can be transformed into its dual form to utilize the kernel trick.\n\n### Dual Form\n\nWARNING: Math-heavy, you can skip this and belive that primal form has the dual form as the equation below (same thing but now the new objective is to optimize alpha, not w!)\n\nBefore transforming from primal to dual form, it is crucial to view the weights as the contributions of all the input samples. Suppose the input sample X has the dimension of [n x m] where n is number of samples and m is number of features. The weight ($w$) on the other hand has the dimension of [m x 1] which m is number of features. This means that by letting $w  = X^T\\\\alpha$, $\\\\alpha$ need to have the dimension of [n x 1]. This implies that $\\\\alpha$ is a set of weights each sample give to contribute to w. The view of the weight is a contribution of the samples also seen from other model such as linear regression as it has the close-form solution of $w=(X^TX)^{-1}X^Ty$ for the weight. Back to SVM, the weight became a weighted sum of all data sample which instead of optimizing the weight, the new goal is to optimize $\\\\alpha_i \\\\forall i$ Interestingly,there is a characteristic that every correctly classified sample and outside margin will have a weight of 0 which means that during inference, only support vector matters.\n\nBy substituting $w = \\\\sum_{i=1}^m \\\\alpha_i y_i x_i$ into the Lagrangian of the constrained soft-margin primal problem and taking derivatives with respect to $w$, $b$, and $\\\\xi$, we obtain the dual formulation: \n\n$$\n\\\\max_{\\\\alpha} \\\\; \\\\sum_{i=1}^{m} \\\\alpha_i - \\\\frac{1}{2} \\\\sum_{i=1}^{m} \\\\sum_{j=1}^{m} \\\\alpha_i \\\\alpha_j y_i y_j \\\\, x_i^T x_j\\\\quad \\\\text{s.t.} \\\\quad     0 \\\\leq \\\\alpha_i \\\\leq C, \\\\; \\\\sum_{i=1}^{m} \\\\alpha_i y_i = 0\n$$\n\n### Kernel trick\n\nIf you made this far, it is fasinating because the rest is packed with visualisations!!!\n\nNow here comes the cool part, now lets make it learn non-linear relationships. Imagine folding your in quarter and cut a line in the middle making a square in the middle, SVM also the same way, it non-linearly maps to anothe dimensional space where you hope for the best the data is linearly seperable and when it maps back to the original space, it will be a non-linear boundary like a curve or a zic-zac pattern. Starting by shooting your sample to the space called phi\n$$\nx_i \\\\mapsto \\\\phi(x_i)\n$$\n\nThen the dual form becomes:\n\n$$\n\\\\begin{aligned}\\\\max_{\\\\alpha} \\\\quad\n& \\\\sum_{i=1}^{m} \\\\alpha_i\n- \\\\frac{1}{2}\n\\\\sum_{i=1}^{m} \\\\sum_{j=1}^{m}\n\\\\alpha_i \\\\alpha_j y_i y_j \\\\, \n\\\\phi(x_i)^{T} \\\\phi(x_j) \\\\\\\\[6pt]\n\\\\text{s.t.} \\\\quad\n& 0 \\\\le \\\\alpha_i \\\\le C, \\\\quad \\\\forall i = 1, \\\\ldots, m, \\\\\\\\[4pt]\n& \\\\sum_{i=1}^{m} \\\\alpha_i y_i = 0\n\\\\end{aligned}\n$$\n\nBut remember, we are optimizing $\\\\alpha$ ! So the term $\\\\phi(x_i)^{T} \\\\phi(x_j)$ does not change the whole time! but recomputing each iteration is very expensive so we precomputed it first then access later! Even better, we dont need to define phi, we create a matrix where:\n\n$$\nK_{ij} = K(x_i, x_j) = \\\\phi(x_i)^{\\\\top} \\\\phi(x_j)\n$$\nWhat a genius idea!\n\nHere is the visualisation of the polynomial kernel (first few slides):\n\n<iframe src=\"https://14522561-svm-presentation.netlify.app/\" width=\"100%\" height=\"600px\" style=\"border: none; border-radius: 8px;\" allowfullscreen></iframe>\n\n\n### Optimization - SMO  \n\nOK now comes the parts that they might not teach you in-class. How do SVM optimize? It will use an algorithm called Sequential Minimal Optimization and below is an visual approach to this algorithm.\n\nIf you carefully inspect the dual formulation enough long you will see that it has the constraint meaning that if you optimize 1 $\\\\alpha$ you need to update another. That leads to this visualization\n\n![alt text](/images/svm-blog/image-1.png)\n\n\nThink of them like a pulley system, you lift one up, you lower one down but you dont want to go to high\n\nYou want to optimize the alplas so that if you pick 2 alphas and optimize them, you will need to optimize another (The sum constraint). Also, you dont want to optimize them too far\n\nNow with multiples alpha the pulley system would look like this\n\n![alt text](/images/svm-blog/image-2.png)\n\nThe red block are locked in place. Each iteration, you will pick 1 alpha sequentially and another randomly to optimize them.\n\nNow what fascinating is that each iteration you need only 1 step to optimize because they follow a quadratic form.\n\n![alt text](/images/svm-blog/image-3.png)\n\nBecause you dont want to respect the 0 to C contraint for both alphas, you will set the lower and higher bound so that yo can stick to the bound if the optimal point is fall out of range!\n\nRepeat the process and TADAAA!!! You made it!\n\n## Final thought\n\nJust remember, SVM is orgimi, you fold the paper and hope for the best :)\n\nThanks so much for reading and stay in touch with me on linkedin!\n    `,\n    date: \"2025-02-02\",\n    readTime: \"15 min read\",\n    category: \"Machine Learning\",\n    tags: [\"SVM\", \"Support Vector Machines\", \"Optimization\", \"Kernel Methods\", \"Visualizations\"],\n    featured: true,\n  },\n];\n\nexport function getBlogPostBySlug(slug: string): BlogPost | undefined {\n  return blogPosts.find((post) => post.slug === slug);\n}\n\nexport function getFeaturedPosts(): BlogPost[] {\n  return blogPosts.filter((post) => post.featured);\n}\n\nexport function getAllCategories(): string[] {\n  const categories = new Set(blogPosts.map((post) => post.category));\n  return Array.from(categories);\n}\n"],"names":[],"mappings":"AAAA,kBAAkB;;;;;;;;;;;AAaX,MAAM,YAAwB;IACnC;QACE,MAAM;QACN,OAAO;QACP,SAAS;QACT,SAAS,CAAC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;IAmIV,CAAC;QACD,MAAM;QACN,UAAU;QACV,UAAU;QACV,MAAM;YAAC;YAAO;YAA2B;YAAgB;YAAkB;SAAiB;QAC5F,UAAU;IACZ;CACD;AAEM,SAAS,kBAAkB,IAAY;IAC5C,OAAO,UAAU,IAAI,CAAC,CAAC,OAAS,KAAK,IAAI,KAAK;AAChD;AAEO,SAAS;IACd,OAAO,UAAU,MAAM,CAAC,CAAC,OAAS,KAAK,QAAQ;AACjD;AAEO,SAAS;IACd,MAAM,aAAa,IAAI,IAAI,UAAU,GAAG,CAAC,CAAC,OAAS,KAAK,QAAQ;IAChE,OAAO,MAAM,IAAI,CAAC;AACpB"}},
    {"offset": {"line": 189, "column": 0}, "map": {"version":3,"sources":["file:///C:/Users/thanh/OneDrive%20-%20UTS/Personal_code/thong-portfolio/src/components/BlogPostContent.tsx/__nextjs-internal-proxy.mjs"],"sourcesContent":["// This file is generated by next-core EcmascriptClientReferenceModule.\nimport { registerClientReference } from \"react-server-dom-turbopack/server\";\nexport const BlogPostContent = registerClientReference(\n    function() { throw new Error(\"Attempted to call BlogPostContent() from the server but BlogPostContent is on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/OneDrive - UTS/Personal_code/thong-portfolio/src/components/BlogPostContent.tsx <module evaluation>\",\n    \"BlogPostContent\",\n);\n"],"names":[],"mappings":";;;;AAAA,uEAAuE;AACvE;;AACO,MAAM,kBAAkB,IAAA,sUAAuB,EAClD;IAAa,MAAM,IAAI,MAAM;AAA8O,GAC3Q,iHACA","ignoreList":[0]}},
    {"offset": {"line": 203, "column": 0}, "map": {"version":3,"sources":["file:///C:/Users/thanh/OneDrive%20-%20UTS/Personal_code/thong-portfolio/src/components/BlogPostContent.tsx/__nextjs-internal-proxy.mjs"],"sourcesContent":["// This file is generated by next-core EcmascriptClientReferenceModule.\nimport { registerClientReference } from \"react-server-dom-turbopack/server\";\nexport const BlogPostContent = registerClientReference(\n    function() { throw new Error(\"Attempted to call BlogPostContent() from the server but BlogPostContent is on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/OneDrive - UTS/Personal_code/thong-portfolio/src/components/BlogPostContent.tsx\",\n    \"BlogPostContent\",\n);\n"],"names":[],"mappings":";;;;AAAA,uEAAuE;AACvE;;AACO,MAAM,kBAAkB,IAAA,sUAAuB,EAClD;IAAa,MAAM,IAAI,MAAM;AAA8O,GAC3Q,6FACA","ignoreList":[0]}},
    {"offset": {"line": 217, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":""}},
    {"offset": {"line": 225, "column": 0}, "map": {"version":3,"sources":["file:///C:/Users/thanh/OneDrive%20-%20UTS/Personal_code/thong-portfolio/src/app/blog/%5Bslug%5D/page.tsx"],"sourcesContent":["import { notFound } from \"next/navigation\";\r\nimport { getBlogPostBySlug, blogPosts } from \"@/data/blog\";\r\nimport { BlogPostContent } from \"@/components/BlogPostContent\";\r\n\r\ninterface BlogPostPageProps {\r\n  params: Promise<{\r\n    slug: string;\r\n  }>;\r\n}\r\n\r\n// Server Component\r\nexport default async function BlogPostPage({ params }: BlogPostPageProps) {\r\n  const { slug } = await params;\r\n  const post = getBlogPostBySlug(slug);\r\n\r\n  if (!post) {\r\n    notFound();\r\n  }\r\n\r\n  return <BlogPostContent post={post} />;\r\n}\r\n\r\n// Generate static paths for all blog posts\r\nexport function generateStaticParams() {\r\n  return blogPosts.map((post) => ({\r\n    slug: post.slug,\r\n  }));\r\n}\r\n"],"names":[],"mappings":";;;;;;;AAAA;AAAA;AACA;AACA;;;;;AASe,eAAe,aAAa,EAAE,MAAM,EAAqB;IACtE,MAAM,EAAE,IAAI,EAAE,GAAG,MAAM;IACvB,MAAM,OAAO,IAAA,sMAAiB,EAAC;IAE/B,IAAI,CAAC,MAAM;QACT,IAAA,+PAAQ;IACV;IAEA,qBAAO,4SAAC,sNAAe;QAAC,MAAM;;;;;;AAChC;AAGO,SAAS;IACd,OAAO,8LAAS,CAAC,GAAG,CAAC,CAAC,OAAS,CAAC;YAC9B,MAAM,KAAK,IAAI;QACjB,CAAC;AACH"}}]
}